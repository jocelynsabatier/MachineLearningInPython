# -*- coding: utf-8 -*-
"""JS 1.3 ML - Train test split.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PqRSYXOBoLtAkEMK_B53zcSPOQl615xF

# Execute the code below
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
import plotly.express as px

link = "https://raw.githubusercontent.com/murpi/wilddata/master/quests/weather2019.csv"
df_weather = pd.read_csv(link)

"""# Scoring and metrics
Last time, you did a multivariate linear regression. But how can you be sure this multivariate linear regression is better than an univariate ? You have to measure it !

## First regression
Let's begin with a first linear regression : create a new column `'predict_from_sun'` whith the prediction of MAX temperature from the SUNHOUR variable.
"""

# Your code here :

from sklearn.linear_model import LinearRegression

X = df_weather[['SUNHOUR']]
y = df_weather['MAX_TEMPERATURE_C']

model_from_sun = LinearRegression().fit(X, y)

# Your model is trained, it has now some attributes.
# Coefficient, it's the "a" in your equation "aX + b"
print("coefficient :",model_from_sun.coef_)

# Interception, it's the "b" in your equation "aX + b"
print("intercept :", model_from_sun.intercept_)

df_weather['predict_from_sun'] = model_from_sun.predict(df_weather[['SUNHOUR']])

df_weather.head()

"""## R2 score
The best possible R2 score is '1', when our prediction predicts perfectly the reality. Let's see what is our R2 score :
"""

# Change the name of the model if it's necessary
model_from_sun.score(X, y)

"""## Let's continue with 2 others regressions
- Second regression : create a new column 'predict_from_min' whith the prediction of MAX temperature from the MIN temperature variable
- Third regression : create a new column 'predict_from_both' whith the prediction of MAX temperature from the both variables (MIN temperature and Sunhours)
"""

# Your code here :
# 2nd regression :

X1 = df_weather[['MIN_TEMPERATURE_C']]
y = df_weather['MAX_TEMPERATURE_C']

model_from_min = LinearRegression().fit(X1, y)

# Your model is trained, it has now some attributes.
# Coefficient, it's the "a" in your equation "aX + b"
print("coefficient :",model_from_min.coef_)

# Interception, it's the "b" in your equation "aX + b"
print("intercept :", model_from_min.intercept_)

df_weather['predict_from_min'] = model_from_min.predict(df_weather[['MIN_TEMPERATURE_C']])

df_weather.head()

# 3rd regression :

X2 = df_weather[['SUNHOUR','MIN_TEMPERATURE_C']]
y = df_weather['MAX_TEMPERATURE_C']

model_from_both = LinearRegression().fit(X2, y)

# Your model is trained, it has now some attributes.
# Coefficient, it's the "a" in your equation "aX + b"
print("coefficient :",model_from_both.coef_)

# Interception, it's the "b" in your equation "aX + b"
print("intercept :", model_from_both.intercept_)

df_weather['predict_from_both'] = model_from_both.predict(df_weather[['SUNHOUR','MIN_TEMPERATURE_C']])

df_weather.head()

"""## Calculate the R2 score of the 2 new predictions
Be careful : if you still use the same "X" name, you will overwrite it.

Which model has the best score ? Do you think it's logical ?
"""

# Your code here :
print(model_from_min.score(X1, y))
print(model_from_both.score(X2, y))


# The second model has the best score. It's logical because it has two inputs and therefore it should be more precise.

"""# Train Test Split
One of biggest problems of Machine learning is : **overfitting**.

To be sure that machine didn't memorize the result, we use the Train Test Split methodology. We keep some data separate (often 25% of our initial dataset). Then we train our model on the 75% (the "Train set").
After, we can calculate a score on the "Test set".

Let's do that !
"""

# Juste read and execute the code below
from sklearn.model_selection import train_test_split

X = df_weather[['SUNHOUR']]
y = df_weather['MAX_TEMPERATURE_C']

# Here, we split our 2 datasets (the variables "X" and the target "y") into 4 datasets X and y for the train set and X and y for the test set.
# We set the size of the train set to 75%. And the rest is for the test set.
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size = 0.75)
print("The length of the initial dataset is :", len(X))
print("The length of the train dataset is   :", len(X_train))
print("The length of the test dataset is    :", len(X_test))

# Here we train the model only on the train dataset.
newmodel = LinearRegression().fit(X_train, y_train)

# And now we compare both scores :
print("\nScore for the Train dataset :", newmodel.score(X_train, y_train))
print("Score for the Test dataset :", newmodel.score(X_test, y_test))

"""## Both scores are very close, there is no overfitting, well done !

What happens if we don't randomize our dataset. Here, the model learns only on the 9 first months.
"""

# Juste read and execute the code below
from sklearn.model_selection import train_test_split

X = df_weather[['MIN_TEMPERATURE_C']]
y = df_weather['MAX_TEMPERATURE_C']

# We set the size of the train set to 75%. And the rest is for the test set.
# We set the split NOT in random.
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.75, shuffle = False)


# Here we train the model only on the train dataset.
newmodel = LinearRegression().fit(X_train, y_train)

# And now we compare both scores :
print("\nScore for the Train dataset :", newmodel.score(X_train, y_train))
print("Score for the Test dataset :", newmodel.score(X_test, y_test))

"""## There is an overfitting !
Indeed, the model get a good score on the Train dataset, because he learned in winter / spring / summer datas. But he gets a bad score in Falls...

# Let's play !
Train a new model with all numeric variables (without your target of course) and try to have a better score than previously.

Remember to split randomly your dataset before training your model.

Display the Test score.
"""

# Your code here :

from sklearn.linear_model import LinearRegression

X = df_weather[['MIN_TEMPERATURE_C',	'WINDSPEED_MAX_KMH', 'TEMPERATURE_MORNING_C', 'TEMPERATURE_NOON_C',	'TEMPERATURE_EVENING_C',	'PRECIP_TOTAL_DAY_MM',	'HUMIDITY_MAX_PERCENT',	'VISIBILITY_AVG_KM', 'WEATHER_CODE_MORNING',	'WEATHER_CODE_NOON',	'WEATHER_CODE_EVENING',	'TOTAL_SNOW_MM',	'UV_INDEX', 'SUNHOUR']]
y = df_weather['MAX_TEMPERATURE_C']


modelLR = LinearRegression().fit(X, y)

# Your model is trained, it has now some attributes.
# Coefficient, it's the "a" in your equation "aX + b"
print("coefficient :",modelLR.coef_)

# Interception, it's the "b" in your equation "aX + b"
print("intercept :", modelLR.intercept_)

from sklearn.model_selection import train_test_split

X = df_weather[['MIN_TEMPERATURE_C',	'WINDSPEED_MAX_KMH', 'TEMPERATURE_MORNING_C', 'TEMPERATURE_NOON_C',	'TEMPERATURE_EVENING_C',	'PRECIP_TOTAL_DAY_MM',	'HUMIDITY_MAX_PERCENT',	'VISIBILITY_AVG_KM', 'WEATHER_CODE_MORNING',	'WEATHER_CODE_NOON',	'WEATHER_CODE_EVENING',	'TOTAL_SNOW_MM',	'UV_INDEX', 'SUNHOUR']]
y = df_weather['MAX_TEMPERATURE_C']

# Here, we split our 2 datasets (the variables "X" and the target "y") into 4 datasets X and y for the train set and X and y for the test set.
# We set the size of the train set to 75%. And the rest is for the test set.
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size = 0.75)
print("The length of the initial dataset is :", len(X))
print("The length of the train dataset is   :", len(X_train))
print("The length of the test dataset is    :", len(X_test))

# Here we train the model only on the train dataset.
newmodel = LinearRegression().fit(X_train, y_train)

# And now we compare both scores :
print("\nScore for the Train dataset :", newmodel.score(X_train, y_train))
print("Score for the Test dataset :", newmodel.score(X_test, y_test))